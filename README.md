# Airflow + PySpark Data Pipeline Project

## Tá»•ng quan

ÄÃ¢y lÃ  má»™t data pipeline theo **Medallion Architecture** (Bronze â†’ Silver â†’ Gold) Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u Meta Kaggle sá»­ dá»¥ng Apache Airflow cho orchestration vÃ  PySpark cho data processing.

### Kiáº¿n trÃºc chÃ­nh

- **Airflow**: Orchestration vá»›i CeleryExecutor, Redis, PostgreSQL
- **Spark Cluster**: Master, Worker, History Server (PySpark 3.5.0)
- **Storage**: Parquet format cho hiá»‡u suáº¥t tá»‘i Æ°u
- **Data Source**: Meta Kaggle datasets (CSV)
- **Development**: JupyterLab cho interactive development

## Cáº¥u trÃºc thÆ° má»¥c

```
â”œâ”€â”€ dags/                  # Airflow DAGs
â”‚   â””â”€â”€ basic/            # Example DAGs
â”œâ”€â”€ jobs/                 # PySpark job scripts
â”‚   â””â”€â”€ meta/            # Meta Kaggle processing jobs
â”œâ”€â”€ libs/                 # Reusable Python utilities
â”œâ”€â”€ configs/              # YAML configuration files
â”œâ”€â”€ data/                 # Data storage
â”‚   â””â”€â”€ meta/
â”‚       â”œâ”€â”€ raw/         # CSV source files
â”‚       â”œâ”€â”€ bronze/      # Raw + metadata (Parquet)
â”‚       â”œâ”€â”€ silver/      # Cleaned data (Parquet)
â”‚       â””â”€â”€ gold/        # Business metrics (Parquet)
â”œâ”€â”€ conf/                 # Spark configuration
â”œâ”€â”€ notebooks/            # Jupyter notebooks
â”œâ”€â”€ tests/                # Unit tests
â””â”€â”€ requirements/         # Documentation & requirements
```

## Data Pipeline Layers

### ğŸ¥‰ Bronze Layer
- **Input**: Raw CSV files tá»« Meta Kaggle
- **Processing**: Schema validation, metadata addition
- **Output**: Parquet files vá»›i `ingest_ts`, `file_name`, `source_system`
- **Partitioning**: Theo `source_table`

### ğŸ¥ˆ Silver Layer
- **Processing**: Data cleaning, deduplication, standardization
- **Features**: Window functions, join enrichment, missing data handling
- **Quality**: Data validation vá»›i rejection patterns

### ğŸ¥‡ Gold Layer
- **Processing**: Business metrics, dimensional modeling
- **Features**: KPI tables, SCD Type 2, fact tables
- **Output**: Ready-to-use data cho BI vÃ  analytics

## YÃªu cáº§u há»‡ thá»‘ng

### Pháº§n má»m
- Docker & Docker Compose
- Python 3.8+
- Java 11 (cho PySpark)

### TÃ i nguyÃªn
- RAM: Tá»‘i thiá»ƒu 8GB (khuyáº¿n nghá»‹ 16GB+)
- CPU: Tá»‘i thiá»ƒu 4 cores
- Disk: Tá»‘i thiá»ƒu 20GB free space

## CÃ i Ä‘áº·t vÃ  cháº¡y

### 1. Clone repository

```bash
git clone <repository-url>
cd airflow-compose
```

### 2. Chuáº©n bá»‹ mÃ´i trÆ°á»ng

```bash
# Copy environment file
cp .env.example .env

# Táº¡o thÆ° má»¥c cáº§n thiáº¿t
mkdir -p data/meta/{raw,bronze,silver,gold}
mkdir -p configs jobs/meta libs tests
```

### 3. Chuáº©n bá»‹ dá»¯ liá»‡u Meta Kaggle

Táº£i xuá»‘ng cÃ¡c file CSV tá»« Kaggle Meta dataset vÃ  Ä‘áº·t vÃ o `data/meta/raw/`:
- `Datasets.csv`
- `Competitions.csv` 
- `Users.csv`
- `Tags.csv`
- `Kernels.csv`

### 4. Build vÃ  khá»Ÿi Ä‘á»™ng services

```bash
# Build custom Airflow image
docker-compose build

# Khá»Ÿi Ä‘á»™ng táº¥t cáº£ services
docker-compose up -d

# Kiá»ƒm tra tráº¡ng thÃ¡i
docker-compose ps
```

### 5. Truy cáº­p cÃ¡c services

- **Airflow UI**: http://localhost:8080 (airflow/airflow)
- **Spark Master UI**: http://localhost:1990
- **Spark History Server**: http://localhost:18080
- **JupyterLab**: http://localhost:8888
- **Flower (Celery Monitor)**: http://localhost:5555 (cáº§n `--profile flower`)

## PhÃ¡t triá»ƒn

### DAG Development

```python
# LuÃ´n sá»­ dá»¥ng configuration-driven approach
from airflow.models import Variable
base_dir = Variable.get("BASE_DATA_DIR", default_var="/opt/airflow/data")

# Load config tá»« YAML
import yaml
with open('/opt/airflow/configs/meta_pipeline_config.yaml', 'r') as f:
    config = yaml.safe_load(f)
```

### Testing

```bash
# Cháº¡y unit tests
docker exec -it <airflow-worker-container> pytest tests/

# Test PySpark jobs
python -m pytest tests/ -v --tb=short
```

### Debugging

```bash
# Xem logs
docker-compose logs -f airflow-scheduler
docker-compose logs -f spark-master

# Access container
docker exec -it <container-name> bash
```

## Data Quality Framework

### Validation Rules
- **Bronze**: Schema compliance, mandatory fields
- **Silver**: Deduplication effectiveness, join success rates  
- **Gold**: Business rule compliance, FK integrity

### Circuit Breakers
- Fail pipeline náº¿u rejection rate > 10%
- Validation checks cho má»i layer
- JSON reports cho táº¥t cáº£ DQ checks

## NguyÃªn táº¯c phÃ¡t triá»ƒn

### âœ… DO
- Sá»­ dá»¥ng YAML configs cho má»i configuration
- Implement proper error handling vÃ  Slack alerts
- Viáº¿t unit tests vá»›i `pytest` + `chispa`
- Sá»­ dá»¥ng `mode('overwrite')` cho idempotency
- Comment code báº±ng tiáº¿ng Viá»‡t cho business logic

### âŒ DON'T
- Hardcode paths, schemas, business rules
- Sá»­ dá»¥ng `inferSchema` (luÃ´n define StructType)
- Skip data quality validation
- Ignore performance considerations (file sizes, partitioning)

## Troubleshooting

### Common Issues

**Container startup failures:**
```bash
# Check resources
docker system df
docker system prune

# Restart clean
docker-compose down
docker-compose up -d
```

**Spark connection issues:**
```bash
# Check network connectivity
docker exec -it airflow-worker ping spark-master

# Verify Spark Master URL
echo $SPARK_MASTER_URL
```

**Volume mount issues:**
```bash
# Check permissions
ls -la data/ logs/
chmod -R 755 data/ logs/
```

## Performance Tuning

### Spark Configuration
- **File sizes**: Target 128-512MB Parquet files
- **Partitioning**: Sá»­ dá»¥ng `run_date` hoáº·c business keys
- **Memory**: Tune executor/driver memory theo workload
- **Parallelism**: Adjust based on cluster resources

### Airflow Optimization
- **Concurrency**: Configure based on resources
- **Retries**: Set appropriate retry delays
- **SLA**: Monitor task execution times

## ÄÃ³ng gÃ³p

1. Fork repository
2. Táº¡o feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Táº¡o Pull Request

## License

This project is licensed under the Apache License 2.0 - see the LICENSE file for details.

## LiÃªn há»‡

- **Project Maintainer**: [TÃªn cá»§a báº¡n]
- **Email**: [Email cá»§a báº¡n]
- **Documentation**: See `.github/copilot-instructions.md` for detailed development guidelines