# Airflow + PySpark Data Pipeline Project

## Tá»•ng quan

ÄÃ¢y lÃ  má»™t data pipeline hoÃ n chá»‰nh theo **Medallion Architecture** (Bronze â†’ Silver â†’ Gold) Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u Meta Kaggle, sá»­ dá»¥ng Apache Airflow cho orchestration vÃ  Apache Spark cluster cho data processing trong mÃ´i trÆ°á»ng containerized.

### Kiáº¿n trÃºc chÃ­nh

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Airflow Web    â”‚    â”‚ Airflow Worker  â”‚    â”‚   PostgreSQL    â”‚
â”‚     (8080)      â”‚â—„â”€â”€â–ºâ”‚   + Celery      â”‚â—„â”€â”€â–ºâ”‚     (5432)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–²                       â–²                       â–²
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      Redis      â”‚
                    â”‚     (6379)      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Spark Master  â”‚    â”‚  Spark Worker   â”‚    â”‚ History Server  â”‚
â”‚     (1990)      â”‚â—„â”€â”€â–ºâ”‚     (8081)      â”‚    â”‚     (18080)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–²                       â–²                       â–²
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   JupyterLab    â”‚
                    â”‚     (8888)      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ThÃ nh pháº§n chÃ­nh:**
- **Airflow**: Orchestration vá»›i CeleryExecutor, Redis, PostgreSQL
- **Spark Cluster**: Master, Worker, History Server (PySpark 3.5.0)
- **Storage**: Parquet format theo Medallion Architecture
- **Data Source**: Meta Kaggle datasets (CSV format)
- **Development**: JupyterLab vá»›i PySpark integration

**TÃ­nh nÄƒng ná»•i báº­t:**
- ğŸš€ **Spark 3.5.0** vá»›i cluster setup hoÃ n chá»‰nh
- ğŸ“Š **JupyterLab** vá»›i PySpark kernel Ä‘Æ°á»£c cáº¥u hÃ¬nh sáºµn
- ğŸ“ˆ **History Server** cho monitoring á»©ng dá»¥ng
- ğŸ”§ **Auto-scaling** worker nodes
- ğŸ’¾ **Persistent storage** cho data vÃ  notebooks
- ğŸ¯ **Health checks** cho táº¥t cáº£ services
- âš™ï¸ **Configuration-driven** qua YAML vÃ  environment variables

## Cáº¥u trÃºc thÆ° má»¥c

```
â”œâ”€â”€ docker-compose.yaml         # Service definitions chÃ­nh
â”œâ”€â”€ .env                       # Environment configuration
â”œâ”€â”€ .env.example              # Environment template
â”œâ”€â”€ dags/                     # Airflow DAGs
â”‚   â””â”€â”€ basic/               # Example DAGs
â”‚       â”œâ”€â”€ 01_hello_world_dag.py
â”‚       â”œâ”€â”€ 02_file_processing_dag.py
â”‚       â””â”€â”€ 03_file_processing_v2_dag.py
â”œâ”€â”€ jobs/                     # PySpark job scripts
â”‚   â””â”€â”€ meta/                # Meta Kaggle processing jobs
â”œâ”€â”€ libs/                     # Reusable Python utilities
â”œâ”€â”€ configs/                  # YAML configuration files
â”œâ”€â”€ data/                     # Data storage (mounted in containers)
â”‚   â””â”€â”€ meta/
â”‚       â”œâ”€â”€ raw/             # CSV source files
â”‚       â”œâ”€â”€ bronze/          # Raw + metadata (Parquet)
â”‚       â”œâ”€â”€ silver/          # Cleaned data (Parquet)
â”‚       â””â”€â”€ gold/            # Business metrics (Parquet)
â”œâ”€â”€ conf/                     # Spark configuration files
â”‚   â”œâ”€â”€ spark-defaults.conf   # Main Spark configuration
â”‚   â”œâ”€â”€ spark-defaults.client.conf # Client-specific settings
â”‚   â”œâ”€â”€ log4j2.properties    # Logging configuration
â”‚   â””â”€â”€ metrics.properties   # Metrics configuration
â”œâ”€â”€ images/                   # Docker image definitions
â”‚   â”œâ”€â”€ airflow-custom/       # Custom Airflow image
â”‚   â”‚   â”œâ”€â”€ Dockerfile        # Airflow vá»›i PySpark dependencies
â”‚   â”‚   â””â”€â”€ requirements.txt  # Python packages
â”‚   â””â”€â”€ custom-notebook/      # Custom Jupyter image
â”‚       â”œâ”€â”€ docker-bake.hcl   # Docker Buildx Bake configuration
â”‚       â”œâ”€â”€ Dockerfile        # Custom Jupyter vá»›i Spark integration
â”‚       â”œâ”€â”€ setup_spark.py    # Spark installation script
â”‚       â””â”€â”€ ipython_kernel_config.py # IPython kernel configuration
â”œâ”€â”€ notebooks/                # Jupyter notebooks
â”œâ”€â”€ events/                   # Spark event logs (for History Server)
â”œâ”€â”€ logs/                     # Airflow logs
â”œâ”€â”€ plugins/                  # Airflow plugins
â”œâ”€â”€ secrets/                  # Secrets vÃ  credentials
â”‚   â””â”€â”€ kaggle/              # Kaggle API credentials
â”œâ”€â”€ terraform/                # Infrastructure as Code
â”‚   â”œâ”€â”€ environments/        # Environment-specific configs
â”‚   â””â”€â”€ modules/             # Reusable Terraform modules
â”œâ”€â”€ boto3_deployment/         # AWS deployment utilities
â””â”€â”€ requirements/             # Documentation & requirements
    â””â”€â”€ meta/                # Meta pipeline requirements
```

## Data Pipeline Layers

### ğŸ¥‰ Bronze Layer
- **Input**: Raw CSV files tá»« Meta Kaggle
- **Processing**: Schema validation, metadata addition
- **Output**: Parquet files vá»›i `ingest_ts`, `file_name`, `source_system`
- **Partitioning**: Theo `source_table`

### ğŸ¥ˆ Silver Layer
- **Processing**: Data cleaning, deduplication, standardization
- **Features**: Window functions, join enrichment, missing data handling
- **Quality**: Data validation vá»›i rejection patterns

### ğŸ¥‡ Gold Layer
- **Processing**: Business metrics, dimensional modeling
- **Features**: KPI tables, SCD Type 2, fact tables
- **Output**: Ready-to-use data cho BI vÃ  analytics

## YÃªu cáº§u há»‡ thá»‘ng

### Pháº§n má»m báº¯t buá»™c
- **Docker** vÃ  **Docker Compose** (phiÃªn báº£n má»›i nháº¥t)
- **Docker Buildx plugin** (cho custom image building)
- **Python 3.8+** (cho local development)
- Java 11 JRE (Ä‘Æ°á»£c cÃ i tá»± Ä‘á»™ng trong container)

### TÃ i nguyÃªn há»‡ thá»‘ng
- **RAM**: Tá»‘i thiá»ƒu 8GB (khuyáº¿n nghá»‹ 16GB+ cho mÃ´i trÆ°á»ng production)
- **CPU**: Tá»‘i thiá»ƒu 4 cores (khuyáº¿n nghá»‹ 8+ cores)
- **Disk**: Tá»‘i thiá»ƒu 20GB free space
- **Ports**: 7077, 1990, 8080, 8888, 18080, 5432, 6379 pháº£i available

## Docker Setup vÃ  Build Options

Dá»± Ã¡n nÃ y há»— trá»£ nhiá»u phÆ°Æ¡ng phÃ¡p deployment Docker Ä‘á»ƒ phÃ¹ há»£p vá»›i cÃ¡c use case vÃ  mÃ´i trÆ°á»ng khÃ¡c nhau.

### PhÆ°Æ¡ng phÃ¡p 1: Quick Start vá»›i Pre-built Images

CÃ¡ch nhanh nháº¥t Ä‘á»ƒ báº¯t Ä‘áº§u lÃ  sá»­ dá»¥ng docker-compose setup máº·c Ä‘á»‹nh:

```bash
# Clone repository
git clone <repository-url>
cd aws-medallion-etl

# Khá»Ÿi Ä‘á»™ng vá»›i pre-built images
docker-compose up -d
```

### PhÆ°Æ¡ng phÃ¡p 2: Building Custom Images

Äá»ƒ tÃ¹y chá»‰nh hoáº·c khi pre-built image khÃ´ng available:

```bash
# Build custom images vÃ  khá»Ÿi Ä‘á»™ng services
docker-compose up -d --build
```

Äiá»u nÃ y sáº½:
1. Build custom Airflow image vá»›i PySpark dependencies
2. Build custom Jupyter image vá»›i Spark integration  
3. Khá»Ÿi Ä‘á»™ng táº¥t cáº£ services (airflow, spark cluster, jupyter)

### PhÆ°Æ¡ng phÃ¡p 3: Sá»­ dá»¥ng Docker Buildx Bake (Advanced)

Äá»ƒ cÃ³ control nhiá»u hÆ¡n vá» build process, dá»± Ã¡n bao gá»“m file `docker-bake.hcl` configuration:

```bash
# Build custom Jupyter image báº±ng Docker Buildx Bake
cd images/custom-notebook
docker buildx bake

# Hoáº·c build specific targets
docker buildx bake custom-notebook

# Khá»Ÿi Ä‘á»™ng services sau khi build
cd ../..
docker-compose up -d
```

#### Hiá»ƒu vá» docker-bake.hcl

File `docker-bake.hcl` Ä‘á»‹nh nghÄ©a multi-stage build process:

```hcl
target "foundation" -> target "base-notebook" -> target "minimal-notebook" -> target "custom-notebook"
```

- **foundation**: Base Python 3.10.12 environment
- **base-notebook**: Basic Jupyter setup tá»« official Docker Stacks
- **minimal-notebook**: Minimal Jupyter environment vá»›i essential packages
- **custom-notebook**: Customized image vá»›i Spark integration

**Lá»£i Ã­ch cá»§a Docker Buildx Bake:**
- âš¡ **Parallel builds**: Multiple targets Ä‘Æ°á»£c build Ä‘á»“ng thá»i
- ğŸ”§ **Layer caching**: Efficient rebuilds vá»›i dependency tracking
- ğŸ“¦ **Official base images**: Built trÃªn trusted Jupyter Docker Stacks
- ğŸ¯ **Targeted builds**: Build chá»‰ nhá»¯ng gÃ¬ cáº§n thiáº¿t

### Environment Configuration

```bash
# Táº¡o environment file
cp .env.example .env

# Chá»‰nh sá»­a configuration theo nhu cáº§u
nano .env
```

**Key environment variables:**
```bash
# Image building
SPARK_VERSION=3.5.0
JUPYTERLAB_VERSION=latest

# Service ports
SPARK_MASTER_WEBUI_PORT=1990
JUPYTER_PORT=8888
SPARK_HISTORY_SERVER_PORT=18080

# Resource allocation
SPARK_WORKER_MEMORY=16G
SPARK_WORKER_CORES=8
SPARK_WORKER_REPLICAS=1
```

### Chuáº©n bá»‹ dá»¯ liá»‡u Meta Kaggle

Táº£i xuá»‘ng cÃ¡c file CSV tá»« Kaggle Meta dataset vÃ  Ä‘áº·t vÃ o `data/meta/raw/`:
- `Datasets.csv`
- `Competitions.csv` 
- `Users.csv`
- `Tags.csv`
- `Kernels.csv`

### Quick Start Steps

```bash
# 1. Clone repository
git clone <repository-url>
cd aws-medallion-etl

# 2. Setup environment
cp .env.example .env

# 3. Táº¡o thÆ° má»¥c cáº§n thiáº¿t
mkdir -p data/meta/{raw,bronze,silver,gold}
mkdir -p configs jobs/meta libs tests

# 4. Choose deployment method:

# Option A: Quick start (sá»­ dá»¥ng pre-built hoáº·c build tá»± Ä‘á»™ng)
docker-compose up -d

# Option B: Force rebuild custom images
docker-compose up -d --build

# Option C: Sá»­ dá»¥ng Docker Buildx Bake (khuyáº¿n nghá»‹ cho development)
cd images/custom-notebook && docker buildx bake && cd ../..
docker-compose up -d
```

### Truy cáº­p cÃ¡c Services

| Service | URL | Description | Credentials |
|---------|-----|-------------|-------------|
| **Airflow Webserver** | http://localhost:8080 | DAG management vÃ  monitoring | airflow/airflow |
| **Spark Master UI** | http://localhost:1990 | Cluster management vÃ  monitoring | - |
| **Spark History Server** | http://localhost:18080 | Completed applications history | - |
| **JupyterLab** | http://localhost:8888 | Interactive development environment | Token: psi |
| **Flower (Celery)** | http://localhost:5555 | Celery task monitoring | Cáº§n `--profile flower` |

### Verification

```bash
# Kiá»ƒm tra táº¥t cáº£ services Ä‘ang cháº¡y
docker-compose ps

# Xem logs tá»•ng quÃ¡t
docker-compose logs -f

# Xem logs cá»§a service cá»¥ thá»ƒ
docker-compose logs -f airflow-scheduler
docker-compose logs -f spark-master
```

## Usage Examples vÃ  Development Workflow

### Cháº¡y Sample Applications

Dá»± Ã¡n bao gá»“m cÃ¡c sample PySpark applications trong `dags/basic/`:

#### 1. Hello World DAG
```bash
# KÃ­ch hoáº¡t DAG tá»« Airflow UI hoáº·c CLI
docker-compose exec airflow-worker airflow dags trigger 01_hello_world_dag
```

#### 2. File Processing DAG
```bash
# Submit file processing job
docker-compose exec airflow-worker airflow dags trigger 02_file_processing_dag
```

#### 3. Advanced File Processing
```bash
# Submit advanced processing job
docker-compose exec airflow-worker airflow dags trigger 03_file_processing_v2_dag
```

### Sá»­ dá»¥ng JupyterLab cho Interactive Development

1. **Truy cáº­p JupyterLab**: http://localhost:8888 (token: psi)
2. **Navigate** Ä‘áº¿n thÆ° má»¥c `work` directory
3. **Táº¡o notebook má»›i** hoáº·c má»Ÿ existing notebooks
4. **PySpark Ä‘Æ°á»£c cáº¥u hÃ¬nh sáºµn** vÃ  ready to use:

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

# SparkSession tá»± Ä‘á»™ng káº¿t ná»‘i Ä‘áº¿n cluster
spark = SparkSession.builder \
    .appName("MetaDataAnalysis") \
    .master("spark://spark-master:7077") \
    .config("spark.executor.memory", "2g") \
    .config("spark.executor.cores", "2") \
    .getOrCreate()

# Example: Load Meta Kaggle datasets
datasets_df = spark.read.option("header", "true") \
    .csv("/opt/airflow/data/meta/raw/Datasets.csv")

datasets_df.show(10)
datasets_df.printSchema()

# Perform some analysis
popular_datasets = datasets_df.groupBy("OwnerUserId") \
    .count() \
    .orderBy(desc("count")) \
    .limit(10)

popular_datasets.show()
```

### Development Workflow

#### 1. Interactive Development Phase
```python
# Trong JupyterLab - PhÃ¡t triá»ƒn vÃ  test logic
from pyspark.sql import SparkSession
import yaml

# Load configuration
with open('/opt/airflow/configs/meta_pipeline_config.yaml', 'r') as f:
    config = yaml.safe_load(f)

# Test transformations interactively
spark = SparkSession.builder.appName("Development").getOrCreate()
# Your development code here...
```

#### 2. PySpark Job Development
```python
# Táº¡o PySpark jobs trong jobs/meta/
# jobs/meta/bronze_processing.py

from pyspark.sql import SparkSession
from pyspark.sql.types import *
import yaml
import sys

def main():
    spark = SparkSession.builder \
        .appName("BronzeProcessing") \
        .getOrCreate()
    
    # Load config
    with open('/opt/airflow/configs/meta_pipeline_config.yaml', 'r') as f:
        config = yaml.safe_load(f)
    
    # Processing logic
    # ...
    
    spark.stop()

if __name__ == "__main__":
    main()
```

#### 3. Airflow DAG Development

```python
# dags/meta_pipeline_dag.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.models import Variable
from datetime import datetime, timedelta

# LuÃ´n sá»­ dá»¥ng configuration-driven approach
base_dir = Variable.get("BASE_DATA_DIR", default_var="/opt/airflow/data")

def run_spark_job(**context):
    """Submit PySpark job to cluster"""
    import subprocess
    
    cmd = [
        "spark-submit",
        "--master", "spark://spark-master:7077",
        "--deploy-mode", "client",
        "/opt/airflow/jobs/meta/bronze_processing.py"
    ]
    
    result = subprocess.run(cmd, capture_output=True, text=True)
    if result.returncode != 0:
        raise Exception(f"Spark job failed: {result.stderr}")
    
    return result.stdout

# DAG definition
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2023, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

with DAG(
    'meta_pipeline',
    default_args=default_args,
    description='Meta Kaggle Data Pipeline',
    schedule_interval='@daily',
    catchup=False,
    tags=['meta', 'kaggle', 'medallion']
) as dag:

    # Bronze layer processing
    bronze_task = PythonOperator(
        task_id='bronze_processing',
        python_callable=run_spark_job
    )
    
    # Silver layer processing  
    silver_task = BashOperator(
        task_id='silver_processing',
        bash_command="""
        spark-submit \
        --master spark://spark-master:7077 \
        --deploy-mode client \
        /opt/airflow/jobs/meta/silver_processing.py
        """
    )
    
    # Task dependencies
    bronze_task >> silver_task
```

### Testing vÃ  Debugging

#### Unit Testing
```bash
# Cháº¡y unit tests trong container
docker-compose exec airflow-worker bash -c "cd /opt/airflow && python -m pytest tests/ -v"

# Test PySpark jobs vá»›i chispa
docker-compose exec airflow-worker bash -c """
cd /opt/airflow && python -c '
from chispa import assert_df_equality
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("Test").getOrCreate()
# Your test code here
'
"""
```

#### Debugging vÃ  Monitoring

```bash
# Xem logs cá»§a cÃ¡c services
docker-compose logs -f airflow-scheduler
docker-compose logs -f airflow-worker  
docker-compose logs -f spark-master
docker-compose logs -f spark-worker

# Access vÃ o container Ä‘á»ƒ debug
docker-compose exec airflow-worker bash
docker-compose exec spark-master bash

# Kiá»ƒm tra Spark applications
curl http://localhost:1990/api/v1/applications
```

#### Performance Monitoring

1. **Live Applications**: http://localhost:1990 - Monitor running jobs
2. **Completed Applications**: http://localhost:18080 - Review completed jobs  
3. **Application Details**: Click vÃ o application IDs trong web UIs
4. **Airflow Monitoring**: http://localhost:8080 - DAG runs vÃ  task details
5. **Celery Monitoring**: http://localhost:5555 - Worker task queues

## Configuration vÃ  Scaling

### Environment Variables

Key configuration options trong `.env`:

```bash
# ==========================================
# SPARK CONFIGURATION
# ==========================================
SPARK_VERSION=3.5.0
SPARK_MASTER_PORT=7077
SPARK_MASTER_WEBUI_PORT=1990
SPARK_HISTORY_SERVER_PORT=18080

# Worker configuration
SPARK_WORKER_MEMORY=16G
SPARK_WORKER_CORES=8
SPARK_WORKER_REPLICAS=1

# Performance tuning
SPARK_EXECUTOR_MEMORY=16g
SPARK_EXECUTOR_CORES=8
SPARK_DRIVER_MEMORY=16g

# ==========================================
# JUPYTER CONFIGURATION
# ==========================================
JUPYTER_PORT=8888
JUPYTER_TOKEN=psi
```

### Scaling Workers

```bash
# Scale Ä‘áº¿n 3 worker nodes
docker-compose up -d --scale spark-worker=3

# Hoáº·c set trong .env file
SPARK_WORKER_REPLICAS=3
docker-compose up -d
```

### Custom Spark Configuration

Chá»‰nh sá»­a `conf/spark-defaults.conf` Ä‘á»ƒ tÃ¹y chá»‰nh Spark settings:

```properties
# Memory vÃ  performance settings
spark.executor.memory               2g
spark.executor.cores                1
spark.driver.memory                 2g
spark.driver.maxResultSize          1g

# Adaptive query execution
spark.sql.adaptive.enabled          true
spark.sql.adaptive.coalescePartitions.enabled true
spark.sql.shuffle.partitions        200

# Serialization
spark.serializer                    org.apache.spark.serializer.KryoSerializer
spark.sql.execution.arrow.pyspark.enabled true
```

## Data Quality Framework

### Validation Rules theo Layer
- **Bronze Layer**: 
  - Schema compliance vá»›i StructType definition
  - Mandatory field validation
  - Data type validation
  - File format validation

- **Silver Layer**: 
  - Deduplication effectiveness testing
  - Join success rate monitoring  
  - Missing data handling validation
  - Data standardization checks

- **Gold Layer**: 
  - Business rule compliance testing
  - Foreign key integrity validation
  - Aggregation accuracy verification
  - KPI calculation validation

### Circuit Breakers vÃ  Monitoring
- **Fail pipeline** náº¿u rejection rate > 10% cho báº¥t ká»³ table nÃ o
- **Validation checks** cho má»i layer vá»›i detailed reporting
- **JSON reports** cho táº¥t cáº£ DQ checks vá»›i pass/fail status
- **XCom integration** Ä‘á»ƒ pass metadata giá»¯a cÃ¡c tasks
- **Slack alerts** cho failures vÃ  data quality issues

### DQ Implementation Example

```python
# libs/data_quality.py
def validate_bronze_layer(df, config, table_name):
    """Validate bronze layer data quality"""
    total_records = df.count()
    
    # Schema validation
    expected_schema = config['schemas'][table_name]
    schema_valid = validate_schema(df.schema, expected_schema)
    
    # Mandatory field validation  
    null_counts = {}
    for field in config['mandatory_fields'][table_name]:
        null_count = df.filter(col(field).isNull()).count()
        null_counts[field] = null_count
    
    # Calculate rejection rate
    rejected_count = sum(null_counts.values())
    rejection_rate = rejected_count / total_records if total_records > 0 else 0
    
    # Circuit breaker
    if rejection_rate > 0.1:  # 10% threshold
        raise Exception(f"Rejection rate {rejection_rate:.2%} exceeds 10% for table {table_name}")
    
    # Generate DQ report
    dq_report = {
        'table_name': table_name,
        'total_records': total_records,
        'rejected_records': rejected_count,
        'rejection_rate': rejection_rate,
        'schema_valid': schema_valid,
        'null_counts': null_counts,
        'status': 'PASS' if rejection_rate <= 0.1 else 'FAIL'
    }
    
    return dq_report
```

## Troubleshooting vÃ  Performance Tuning

### Common Issues

#### Services khÃ´ng khá»Ÿi Ä‘á»™ng Ä‘Æ°á»£c:
```bash
# Kiá»ƒm tra ports cÃ³ available khÃ´ng
netstat -tulpn | grep -E "(1990|8888|18080|7077|8080|5432|6379)"

# Restart services
docker-compose down && docker-compose up -d

# Kiá»ƒm tra Docker resources
docker system df
```

#### Worker nodes khÃ´ng join cluster:
```bash
# Kiá»ƒm tra worker logs
docker-compose logs spark-worker

# Verify network connectivity
docker-compose exec spark-worker ping spark-master

# Kiá»ƒm tra memory settings
docker-compose exec spark-worker free -h
```

#### Build failures:
```bash
# Clean Docker build cache
docker builder prune -a

# Rebuild without cache
docker-compose build --no-cache

# For Buildx Bake
cd images/custom-notebook
docker buildx bake --no-cache
```

#### Memory issues:
```bash
# Check Docker resources
docker stats

# Clean up unused resources  
docker system prune -a

# Adjust memory limits trong .env
SPARK_WORKER_MEMORY=8G
SPARK_EXECUTOR_MEMORY=4g
```

### Performance Tuning

#### Resource Allocation Strategies

```bash
# High-memory setup (for large datasets)
SPARK_WORKER_MEMORY=32G  
SPARK_WORKER_CORES=8
SPARK_EXECUTOR_MEMORY=8g
SPARK_DRIVER_MEMORY=4g

# Many small tasks setup
SPARK_WORKER_REPLICAS=4
SPARK_EXECUTOR_CORES=2
SPARK_EXECUTOR_MEMORY=2g

# Memory-optimized setup
SPARK_EXECUTOR_MEMORY_FRACTION=0.8
SPARK_DRIVER_MEMORY_FRACTION=0.8
```

#### Spark Configuration Tuning

```properties
# conf/spark-defaults.conf

# Adaptive query execution (Spark 3.0+)
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.skewJoin.enabled=true

# Memory management
spark.executor.memoryFraction=0.8
spark.sql.adaptive.shuffle.targetPostShuffleInputSize=64MB
spark.sql.adaptive.advisoryPartitionSizeInBytes=128MB

# Serialization performance
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.sql.execution.arrow.pyspark.enabled=true

# Network vÃ  I/O optimization
spark.network.timeout=120s
spark.sql.broadcastTimeout=36000
spark.storage.level=MEMORY_AND_DISK_SER
```

#### Monitoring vÃ  Optimization Best Practices

1. **Resource Monitoring**:
   ```bash
   # Monitor Docker container stats
   docker stats
   
   # Monitor Spark cluster resources
   curl http://localhost:1990/api/v1/applications
   ```

2. **Performance Profiling**:
   - Sá»­ dá»¥ng Spark History Server Ä‘á»ƒ analyze completed jobs
   - Check stage execution times vÃ  identify bottlenecks
   - Monitor shuffle operations vÃ  optimize partitioning

3. **Data Optimization**:
   - Partition data theo business keys (e.g., `run_date`)
   - Target 128-512MB Parquet file sizes
   - Use appropriate compression (snappy, gzip, lz4)

## NguyÃªn táº¯c phÃ¡t triá»ƒn Enterprise

### âœ… REQUIRED PRACTICES
- **Configuration-driven**: Sá»­ dá»¥ng YAML configs cho má»i configuration, paths, schemas
- **Error handling**: Implement proper exception handling vÃ  Slack alerts
- **Testing**: Viáº¿t comprehensive unit tests vá»›i `pytest` + `chispa`
- **Idempotency**: Sá»­ dá»¥ng `mode('overwrite')` vá»›i partitioning cho safe re-runs
- **Vietnamese comments**: Comment code báº±ng tiáº¿ng Viá»‡t cho business logic vÃ  complex transformations
- **Data quality**: Implement validation checks vá»›i circuit breakers
- **Observability**: Add logging, XCom usage, ETL metadata tracking

### âœ… MEDALLION ARCHITECTURE COMPLIANCE
- **Bronze**: Raw data + metadata, schema enforcement, rejection patterns
- **Silver**: Deduplication, cleaning, enrichment vá»›i proper joins
- **Gold**: Business metrics, dimensional modeling, SCD Type 2

### âŒ PROHIBITED PRACTICES  
- **Hardcoding**: Never hardcode paths, schemas, business rules trong code
- **Schema inference**: KhÃ´ng sá»­ dá»¥ng `inferSchema` - luÃ´n define explicit StructType
- **Skip validation**: KhÃ´ng Ä‘Æ°á»£c skip data quality validation checks
- **Unsafe operations**: Avoid non-idempotent operations hoáº·c operations khÃ´ng thá»ƒ re-run
- **Poor error handling**: KhÃ´ng catch exceptions hoáº·c fail silently
- **Missing documentation**: Code pháº£i cÃ³ proper Vietnamese comments cho business logic

### ğŸ—ï¸ DEVELOPMENT METHODOLOGY (15 Steps)

#### Phase 1: Setup & Configuration (Steps 1-5)
1. **Business documentation**: Táº¡o detailed README vá»›i objectives vÃ  KPIs
2. **EDA (Exploratory Data Analysis)**: Analyze data vá»›i PySpark Ä‘á»ƒ hiá»ƒu schema vÃ  patterns  
3. **Configuration creation**: Build comprehensive `configs/meta_pipeline_config.yaml`
4. **Environment setup**: Configure Docker, Spark, Airflow environments
5. **Data preparation**: Download vÃ  organize source data

#### Phase 2: Bronze Layer (Steps 6-7)  
6. **Bronze ingestion**: Load CSV to Parquet vá»›i metadata columns vÃ  schema validation
7. **Bronze DQ checks**: Implement data quality validation vá»›i rejection patterns

#### Phase 3: Silver Layer (Steps 8-9)
8. **Data cleaning**: Deduplication, standardization, logical filtering
9. **Data enrichment**: Joins giá»¯a tables, derived columns, missing data handling

#### Phase 4: Gold Layer (Steps 10-11)
10. **KPI aggregations**: Build business metrics vÃ  summary tables
11. **Dimensional modeling**: Create facts vÃ  dimensions vá»›i SCD Type 2

#### Phase 5: Orchestration & Testing (Steps 12-15)
12. **Airflow DAGs**: Build pipeline orchestration vá»›i proper dependencies
13. **Monitoring setup**: Implement XCom usage, metadata tracking, Slack alerts  
14. **Unit testing**: Comprehensive testing vá»›i `pytest` + `chispa`
15. **Documentation**: Complete architecture docs, troubleshooting guides
- Ignore performance considerations (file sizes, partitioning)

## Troubleshooting

### Common Issues

**Container startup failures:**
```bash
# Check resources
docker system df
docker system prune

# Restart clean
docker-compose down
docker-compose up -d
```

**Spark connection issues:**
```bash
# Check network connectivity
docker exec -it airflow-worker ping spark-master

# Verify Spark Master URL
echo $SPARK_MASTER_URL
```

**Volume mount issues:**
```bash
# Check permissions
ls -la data/ logs/
chmod -R 755 data/ logs/
```

## Performance Tuning

### Spark Configuration
- **File sizes**: Target 128-512MB Parquet files
- **Partitioning**: Sá»­ dá»¥ng `run_date` hoáº·c business keys
- **Memory**: Tune executor/driver memory theo workload
- **Parallelism**: Adjust based on cluster resources

### Airflow Optimization
- **Concurrency**: Configure based on resources
- **Retries**: Set appropriate retry delays
- **SLA**: Monitor task execution times

## Advanced Features

### Infrastructure as Code (Terraform)

Dá»± Ã¡n bao gá»“m Terraform modules Ä‘á»ƒ deploy lÃªn AWS:

```bash
# Navigate to terraform directory
cd terraform/environments/dev

# Initialize Terraform
terraform init

# Plan deployment
terraform plan

# Deploy infrastructure
terraform apply
```

**Terraform modules available:**
- **S3**: Data lake storage vá»›i proper lifecycle policies
- **Glue**: Data catalog vÃ  ETL jobs
- **Redshift**: Data warehouse cho Gold layer
- **IAM**: Roles vÃ  policies cho secure access

### AWS Integration (boto3_deployment)

```bash
# Deploy using boto3 utilities
cd boto3_deployment

# Run deployment
python deploy.py --env dev

# Check deployment status
python resource_inspector.py

# Generate destroy policy
python generate_destroy_policy.py
```

### Monitoring vÃ  Alerting

#### Slack Integration
```python
# dags/meta_pipeline_dag.py
from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator

def task_fail_slack_alert(context):
    slack_msg = f"""
    :red_circle: Task Failed
    *Task*: {context.get('task_instance').task_id}  
    *Dag*: {context.get('task_instance').dag_id}
    *Execution Time*: {context.get('execution_date')}  
    *Log Url*: {context.get('task_instance').log_url}
    """
    
    failed_alert = SlackWebhookOperator(
        task_id='slack_failed',
        http_conn_id='slack_connection',
        message=slack_msg
    )
    return failed_alert.execute(context=context)

# Add to DAG default_args
default_args = {
    'on_failure_callback': task_fail_slack_alert,
    # ... other args
}
```

#### Health Checks
```bash
# Add health check endpoints
curl http://localhost:8080/health  # Airflow health
curl http://localhost:1990/api/v1/applications  # Spark health
curl http://localhost:8888/api  # JupyterLab health
```

## Cleanup vÃ  Maintenance

### Regular Cleanup
```bash
# Stop services
docker-compose down

# Clean up logs (optional)
docker-compose down && rm -rf logs/*

# Remove volumes (CAUTION: deletes data!)
docker-compose down -v

# Remove all images
docker-compose down --rmi all

# Clean up Docker system
docker system prune -a
```

### Backup vÃ  Recovery
```bash
# Backup important data
mkdir -p backups/$(date +%Y-%m-%d)
cp -r data/ backups/$(date +%Y-%m-%d)/
cp -r configs/ backups/$(date +%Y-%m-%d)/

# Backup Docker volumes
docker run --rm -v spark-recovery:/source -v $(pwd)/backups:/backup alpine tar czf /backup/spark-recovery-$(date +%Y-%m-%d).tar.gz -C /source .
```

## Production Deployment Considerations

### Security Hardening
```bash
# Set secure passwords trong .env
AIRFLOW__WEBSERVER__SECRET_KEY="your-secure-secret-key"
JUPYTER_TOKEN="your-secure-token"

# Enable Spark authentication
SPARK_AUTHENTICATION_ENABLED=yes
SPARK_ENCRYPTION_ENABLED=yes
```

### Performance Optimization
```bash
# Production resource allocation
SPARK_WORKER_MEMORY=64G
SPARK_WORKER_CORES=16
SPARK_WORKER_REPLICAS=4
SPARK_EXECUTOR_MEMORY=16g
SPARK_DRIVER_MEMORY=8g
```

### Monitoring Stack
- **Prometheus**: Metrics collection
- **Grafana**: Visualization dashboards  
- **ELK Stack**: Centralized logging
- **AlertManager**: Alert routing

## References vÃ  Documentation

### Official Documentation
- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [Jupyter Docker Stacks](https://jupyter-docker-stacks.readthedocs.io/en/latest/)
- [Docker Buildx Bake Reference](https://docs.docker.com/engine/reference/commandline/buildx_bake/)

### Custom Images vÃ  Extensions
Project nÃ y build dá»±a trÃªn official Jupyter Docker Stacks. ThÃ´ng tin vá»:
- **Creating custom Jupyter images**: [Jupyter Docker Stacks - Using Custom Images](https://jupyter-docker-stacks.readthedocs.io/en/latest/using/custom-images.html)
- **Available base images**: [Jupyter Docker Stacks - Selecting an Image](https://jupyter-docker-stacks.readthedocs.io/en/latest/using/selecting.html)
- **Docker Buildx Bake**: [Docker Buildx Bake Documentation](https://docs.docker.com/build/customize/bake/)

### Data Engineering Resources
- [Medallion Architecture Best Practices](https://docs.databricks.com/lakehouse/medallion.html)
- [PySpark Performance Tuning](https://spark.apache.org/docs/latest/tuning.html)
- [Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)

### AWS Integration
- [AWS Glue Documentation](https://docs.aws.amazon.com/glue/)
- [Amazon S3 Best Practices](https://docs.aws.amazon.com/AmazonS3/latest/userguide/best-practices.html)
- [Amazon Redshift Documentation](https://docs.aws.amazon.com/redshift/)

## Contributing

1. **Fork repository** vÃ  create feature branch
2. **Follow development guidelines** trong `.github/copilot-instructions.md`
3. **Add comprehensive tests** vá»›i pytest + chispa
4. **Test vá»›i sample applications** vÃ  verify all layers
5. **Submit pull request** vá»›i detailed description

### Development Guidelines
- TuÃ¢n thá»§ **Medallion Architecture** patterns
- Implement **configuration-driven** development
- Add **Vietnamese comments** cho business logic
- Follow **15-step methodology** cho complex features
- Ensure **idempotent operations** vá»›i proper testing

## License

This project is licensed under the Apache License 2.0 - see the LICENSE file for details.

## Support

Äá»ƒ Ä‘Æ°á»£c há»— trá»£ vÃ  giáº£i quyáº¿t váº¥n Ä‘á»:
- **Kiá»ƒm tra** [troubleshooting section](#troubleshooting-vÃ -performance-tuning)
- **Review logs** sá»­ dá»¥ng `docker-compose logs`
- **Tham kháº£o** [Jupyter Docker Stacks documentation](https://jupyter-docker-stacks.readthedocs.io/)
- **Check** `.github/copilot-instructions.md` cho detailed development guidelines
- **Create issue** trong repository vá»›i detailed logs vÃ  error messages

---

**Happy Data Pipeline Development! ğŸš€ğŸ“Š**