# Meta Kaggle Pipeline - Boto3 Deployment System

## ğŸ“‹ Overview

Há»‡ thá»‘ng deployment infrastructure cho Meta Kaggle Pipeline sá»­ dá»¥ng Python boto3, tÆ°Æ¡ng tá»± hoÃ n toÃ n vá»›i Terraform modules nhÆ°ng thuáº§n Python. Triá»ƒn khai Medallion Architecture (Bronze/Silver/Gold) trÃªn AWS vá»›i S3, Glue, Athena, vÃ  Redshift Serverless.

## ğŸ—ï¸ Architecture

### Medallion Data Lake Architecture
```
S3 Data Lake (psi-de-glue-congdinh)
â”œâ”€â”€ meta/
â”‚   â”œâ”€â”€ raw/          # CSV source files
â”‚   â”œâ”€â”€ bronze/       # Parquet + metadata  
â”‚   â”œâ”€â”€ silver/       # Cleaned & enriched
â”‚   â””â”€â”€ gold/         # Dimensional model & KPIs
â”œâ”€â”€ scripts/
â””â”€â”€ athena-results/
```

### AWS Services
- **S3**: Data Lake storage vá»›i lifecycle policies
- **IAM**: Roles, policies, auto-granting permissions
- **Glue**: Data Catalog, Crawlers, ETL jobs
- **Athena**: Query engine & analytics workgroup
- **Redshift Serverless**: Data warehouse vá»›i cost control

## ğŸš€ Quick Start

### Prerequisites
```bash
# Install dependencies
pip install boto3 pyyaml

# Configure AWS credentials
aws configure
# hoáº·c export AWS_ACCESS_KEY_ID=xxx
# export AWS_SECRET_ACCESS_KEY=xxx
```

### Basic Deployment
```bash
# Clone vÃ  navigate
cd boto3_deployment

# Set Redshift password
export REDSHIFT_MASTER_PASSWORD="MeoMeo2025!"

# Full deployment (dev environment)
python deploy.py

# Foundation only (S3 + IAM)
python deploy.py --foundation-only

# Validate config trÆ°á»›c khi deploy
python deploy.py --validate

# Show plan without deployment
python deploy.py --plan

# Deploy to production
python deploy.py --environment prod
```

## ğŸ“ Project Structure

```
boto3_deployment/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ dev.yaml              # Development configuration
â”‚   â”œâ”€â”€ staging.yaml          # Staging configuration 
â”‚   â”œâ”€â”€ prod.yaml             # Production configuration
â”‚   â””â”€â”€ config_manager.py     # Configuration loader
â”œâ”€â”€ managers/
â”‚   â”œâ”€â”€ base_manager.py       # Base infrastructure class
â”‚   â”œâ”€â”€ s3_manager.py         # S3 Data Lake manager
â”‚   â”œâ”€â”€ iam_manager.py        # IAM roles & policies
â”‚   â”œâ”€â”€ glue_manager.py       # Glue Data Catalog
â”‚   â””â”€â”€ redshift_manager.py   # Redshift Serverless
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ validator.py          # Deployment validator
â”œâ”€â”€ logs/                     # Deployment & validation logs
â”œâ”€â”€ deploy.py                 # Main deployment script
â””â”€â”€ README.md                 # This file
```

## âš™ï¸ Configuration

### Environment Variables
```bash
# Required
export REDSHIFT_MASTER_PASSWORD="MeoMeo2025!"

# Optional overrides
export AWS_REGION="us-west-1"
export S3_BUCKET_NAME="your-custom-bucket"
export ENVIRONMENT="dev"
export AUTO_DEPLOY_ADVANCED_SERVICES="true"
```

### Configuration Files
Edit `config/dev.yaml`:

```yaml
# General Configuration
aws_region: "us-west-1"
environment: "dev"
s3_bucket_name: "psi-de-glue-congdinh"

# Redshift Configuration
redshift_database_name: "meta_warehouse"
redshift_master_username: "meta_admin"
redshift_base_capacity_rpus: 8
redshift_max_capacity_rpus: 64

# Cost Control
enable_redshift_usage_limits: true
redshift_monthly_usage_limit: 500
redshift_weekly_usage_limit: 125

# Feature Flags
auto_deploy_advanced_services: true
enable_cost_optimization: true
```

## ğŸ¯ Deployment Options

### 1. Foundation Only (`--foundation-only`)
Deploy cÆ¡ báº£n S3 + IAM:
- S3 bucket vá»›i Medallion folder structure
- IAM roles cho Glue, Redshift
- IAM user cho Airflow
- Auto-granting Meta Pipeline policies

```bash
python deploy.py --foundation-only
```

### 2. Full Pipeline (Default)
Deploy complete infrastructure:
- Foundation (S3 + IAM)
- Glue databases, crawlers, jobs
- Athena workgroup  
- Redshift Serverless namespace & workgroup
- Usage limits vÃ  cost control

```bash
python deploy.py
```

### 3. Validation Mode (`--validate`)
Validate configuration without creating resources:

```bash
python deploy.py --validate
```

### 4. Plan Mode (`--plan`)
Show planned resources without deployment:

```bash  
python deploy.py --plan
```

## ğŸ”§ Advanced Usage

### Multi-Environment Deployment
```bash
# Development
python deploy.py --environment dev

# Staging 
python deploy.py --environment staging

# Production
python deploy.py --environment prod
```

### Custom Configuration Directory
```bash
python deploy.py --config-dir /path/to/custom/config
```

### Verbose Logging
```bash
python deploy.py --verbose
```

## âœ… Deployment Success Report

**Deployment Results Summary:**
- âœ… **S3 Data Lake**: Bucket created vá»›i 30 folders, lifecycle rules, intelligent tiering
- âœ… **IAM Resources**: Glue role, Redshift role, Airflow user, auto-granted policies
- âœ… **Glue Data Catalog**: 3 databases (bronze/silver/gold), 18 crawlers, 10 ETL jobs
- âš ï¸ **Athena Workgroup**: Requires additional permissions (workaround available)
- âš ï¸ **Redshift Serverless**: Requires VPC configuration (foundation ready)

**What works perfectly:**
- S3 Medallion Architecture vá»›i cost optimization
- IAM auto-granting permissions system  
- Complete Glue Data Catalog setup
- Configuration management & validation
- Error handling & logging

## ğŸ¯ Quick Success Path

**For immediate production use:**
```bash
# After deployment
python utils/validator.py --environment dev

# With specific deployment file
python utils/validator.py --deployment-file deployment_state_dev_1234567890.json
```

### Validation Checks
- **S3**: Bucket existence, versioning, encryption, folder structure
- **IAM**: Roles, users, policies, attachments
- **Glue**: Databases, crawlers, jobs, Athena workgroup
- **Redshift**: Namespace, workgroup, usage limits, connectivity
- **Connectivity**: AWS credentials, service permissions

## ğŸ“Š Deployment Output Example

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸš€ Meta Kaggle Pipeline Deployment (Boto3)                   â•‘
â•‘                        Infrastructure as Code with Python                        â•‘
â•‘                                                                                  â•‘
â•‘  Environment: dev                                                                â•‘
â•‘  Region:      us-west-1                                                          â•‘
â•‘  Bucket:      psi-de-glue-congdinh                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ” Validating configuration...
âœ“ S3 configuration valid
âœ“ IAM configuration valid
âœ“ GLUE configuration valid
âœ“ REDSHIFTSERVERLESS configuration valid
âœ… Configuration validation passed

ğŸ—ï¸ Starting foundation deployment...
ğŸ“¦ Deploying S3 Data Lake...
âœ“ Created bucket: psi-de-glue-congdinh
âœ“ Created 25 folders
ğŸ” Deploying IAM resources...
âœ“ Created Glue execution role: meta-dev-glue-execution-role
âœ“ Created Redshift service role: meta-dev-redshift-service-role
âœ“ Created Airflow user: meta-dev-airflow-user
âœ“ Auto-attached Meta Kaggle Pipeline policy to user: congdinh
âœ… Foundation deployment completed successfully

ğŸš€ Starting advanced services deployment...
ğŸ•·ï¸ Deploying Glue resources...
âœ“ Created Glue database: meta_bronze
âœ“ Created Glue database: meta_silver
âœ“ Created Glue database: meta_gold
âœ“ Created 15 crawlers
âœ“ Created Athena workgroup: meta-analytics
ğŸ¢ Deploying Redshift Serverless...
âœ“ Created Redshift namespace: meta-dev-namespace
âœ“ Created Redshift workgroup: meta-dev-workgroup
âœ“ Created monthly usage limit: 500 RPU-hours
âœ“ Created weekly usage limit: 125 RPU-hours
âœ… Advanced services deployment completed successfully
```

## ğŸ› ï¸ Troubleshooting

### Common Issues

#### 1. Redshift Password Error
```
Error: Redshift password must be 8-64 characters
```
**Solution**: Set secure password via environment variable:
```bash
export REDSHIFT_MASTER_PASSWORD="MetaKaggle2024!"
```

#### 2. S3 Bucket Already Exists
```
Error: Bucket already exists in different region
```
**Solution**: Update bucket name in config:
```yaml
s3_bucket_name: "your-unique-bucket-name"
```

#### 3. IAM Permissions Error
```
Error: User: arn:aws:iam::xxx:user/xxx is not authorized to perform: iam:CreateRole
```
**Solution**: Ensure AWS user has admin permissions or required IAM policies.

#### 4. Athena CreateWorkGroup Permission Error
```
Error: You are not authorized to perform: athena:CreateWorkGroup on the resource
```
**Solution**: Add Athena permissions to your IAM user:
```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "athena:*"
            ],
            "Resource": "*"
        }
    ]
}
```
**Workaround**: Deploy foundation vÃ  Glue services only, skip Athena:
```bash
python deploy.py --foundation-only
# Then manually create Athena workgroup trong AWS Console
```

#### 5. Redshift Workgroup Creation Failed
```
Error: Could not create workgroup: ValidationException
```
**Solution**: Check subnet IDs vÃ  security groups in VPC configuration.

### Debug Commands
```bash
# Enable debug logging
python deploy.py --verbose

# Check AWS credentials
aws sts get-caller-identity

# Validate without deployment
python deploy.py --validate

# Check resource status
python utils/validator.py --environment dev
```

### Log Files
- Deployment logs: `logs/deployment_{environment}.log`
- Validation logs: `validation_report_{environment}_{timestamp}.json`

## ï¿½ï¸ Destroying Infrastructure

### Safe Resource Cleanup

**âš ï¸ WARNING: Destruction is permanent and cannot be undone!**

The boto3 deployment system includes comprehensive destruction capabilities Ä‘á»ƒ safely remove all AWS resources vÃ  avoid ongoing costs.

### Basic Destruction

```bash
# Destroy all resources except S3 data
python destroy.py --environment dev

# Destroy everything including S3 bucket and ALL data
python destroy.py --environment dev --delete-s3-data

# Skip confirmation prompt (for automation)
python destroy.py --environment dev --yes

# Use specific deployment state file
python destroy.py --deployment-file logs/deployment_state_dev_1234567890.json
```

### Destruction Order

Resources are destroyed in reverse dependency order Ä‘á»ƒ avoid conflicts:

1. **Redshift Serverless** (usage limits â†’ workgroup â†’ namespace)
2. **Glue Resources** (Athena workgroup â†’ jobs â†’ crawlers â†’ databases)  
3. **IAM Resources** (detach policies â†’ delete users â†’ delete roles â†’ delete policies)
4. **S3 Resources** (optional: empty bucket â†’ delete bucket)

### Safety Features

- **Confirmation Required**: User must type 'DELETE' to confirm
- **Resource Discovery**: Automatically finds resources even without deployment state
- **Error Tolerance**: Continues destruction even if some resources fail
- **Comprehensive Logging**: Detailed logs of all destruction activities
- **S3 Protection**: Data preserved by default (requires explicit --delete-s3-data flag)

### Destruction Examples

**Development Environment:**
```bash
python destroy.py --environment dev
```

**Production Environment:**
```bash
python destroy.py --environment prod --delete-s3-data
```

**Automated Cleanup (CI/CD):**
```bash
python destroy.py --environment staging --yes
```

### What Gets Destroyed

âœ… **Always Destroyed:**
- Redshift Serverless namespace, workgroup, usage limits
- Glue databases, crawlers, ETL jobs
- Athena workgroups
- IAM roles, policies, service users
- Policy attachments from current user

âš ï¸ **Conditionally Destroyed:**
- S3 bucket and data (only with `--delete-s3-data` flag)

### Error Handling

The destroyer handles common scenarios gracefully:
- Resources already deleted
- Missing permissions
- Resource dependencies
- Network timeouts

### Cost Savings

Regular cleanup prevents unexpected charges from:
- Redshift Serverless compute usage
- S3 storage fees
- Data transfer costs
- IAM entity limits

## ğŸ” Resource Inspection

### Check Current Resources

Before destroying or for troubleshooting, inspect current AWS resources:

```bash
# Full resource inspection
python resource_inspector.py --environment dev

# Inspect specific service only
python resource_inspector.py --environment dev --service s3
python resource_inspector.py --environment dev --service iam
python resource_inspector.py --environment dev --service glue
python resource_inspector.py --environment dev --service redshift

# Save inspection report to file
python resource_inspector.py --environment dev --output-file resource_report.json
```

### Example Output

```
ğŸ” Meta Kaggle Pipeline Resource Inspector
Environment: dev
Region: us-west-1
================================================================================

ğŸ“¦ S3 Resources:
  âœ… Bucket: psi-de-glue-congdinh
     Objects: 156
     Size: 45.8 MB
     Versioning: Enabled

ğŸ” IAM Resources:
  Roles: 2/2
  Policies: 4/4
  Users: 1/1

ğŸ•·ï¸ Glue Resources:
  Databases: 3/3
  Crawlers: 18
  Jobs: 10
  Athena Workgroup: âœ… Found

ğŸ¢ Redshift Serverless:
  Namespace: âœ… Found
  Workgroup: âœ… Found
  Usage Limits: 2
  JDBC URL: jdbc:redshift://meta-dev-workgroup.215103618168.us-west-1.redshift-serverless.amazonaws.com:5439/meta_warehouse
================================================================================
```

### Use Cases for Inspection

- **Pre-deployment**: Verify clean environment
- **Post-deployment**: Confirm all resources created
- **Troubleshooting**: Identify missing or misconfigured resources
- **Cost monitoring**: Track resource usage and growth
- **Cleanup verification**: Ensure resources properly destroyed

## ğŸ’° Cost Optimization
```yaml
# config/dev.yaml
enable_redshift_usage_limits: true
redshift_monthly_usage_limit: 500    # ~$125-225/month
redshift_weekly_usage_limit: 125     # Weekly cap
redshift_usage_limit_breach_action: "log"  # log, emit-metric, deactivate
```

### S3 Lifecycle Policies
Automatic cost optimization vá»›i intelligent tiering:
- Raw data: Standard â†’ IA (30d) â†’ Glacier (90d) â†’ Deep Archive (365d)
- Bronze: Standard â†’ IA (60d) â†’ Glacier (180d)
- Silver: Standard â†’ IA (90d) â†’ Glacier (365d)
- Athena results: Auto-delete after 30 days
- Rejected records: Auto-delete after 90 days

### Estimated Costs (us-west-1)
- **S3**: ~$10-30/month (depending on data volume)
- **Redshift Serverless**: ~$125-225/month (vá»›i usage limits)
- **Glue**: $0.44/DPU-hour (pay per job)
- **Athena**: $5/TB scanned

## ğŸ”„ Comparison vá»›i Terraform

| Feature | Terraform | Boto3 Deployment |
|---------|-----------|-------------------|
| Configuration | HCL + tfvars | Python + YAML |
| State Management | terraform.tfstate | JSON deployment logs |
| Resource Management | terraform apply/destroy | Python classes |
| Validation | terraform plan/validate | Built-in validators |
| Deployment Speed | Medium | Fast |
| Debugging | terraform logs | Python logging |
| Customization | Limited by providers | Full Python flexibility |
| Learning Curve | Terraform-specific | Standard Python |

## ğŸ” Security Features

### Auto-Granting Permissions
System tá»± Ä‘á»™ng attach Meta Kaggle Pipeline policy vÃ o current user:
```python
# Tá»« IAMManager
def create_meta_kaggle_pipeline_policy(self):
    # Táº¡o comprehensive policy
    policy_arn = self._create_policy(policy_name, policy_document)
    
    # Auto-attach vÃ o current user
    if self.current_user_info.get('username'):
        self._attach_policy_to_user(self.current_user_info['username'], policy_arn)
```

### Secure Password Generation
```python
def generate_secure_redshift_password(self):
    # AWS Redshift requirements:
    # - 8-64 characters, uppercase, lowercase, digits
    # - No forbidden characters: / @ " space \ '
    password = generate_with_requirements()
    return password
```

### S3 Security
- Server-side encryption (AES256)
- Public access blocked
- Versioning enabled
- Lifecycle policies

## ğŸ“ Learning Path

### 1. Understand Architecture
- Review Medallion Architecture concepts
- Study AWS services interaction
- Compare vá»›i Terraform implementation

### 2. Configuration Management
- Edit `config/dev.yaml` 
- Understand environment variables
- Practice different deployment modes

### 3. Deployment Practice
```bash
# Start vá»›i validation
python deploy.py --validate

# Try foundation only
python deploy.py --foundation-only

# Full deployment
python deploy.py
```

### 4. Monitoring & Validation
```bash
# Run validation
python utils/validator.py

# Check AWS Console
# - S3 bucket structure
# - IAM roles vÃ  policies
# - Glue databases
# - Redshift Serverless
```

### 5. Troubleshooting Practice
- Simulate common errors
- Practice debugging commands
- Understand log analysis

## ğŸ“š References

- [AWS Medallion Architecture Guide](https://docs.aws.amazon.com/whitepapers/latest/building-data-lakes/building-data-lake-on-aws.html)
- [Redshift Serverless Documentation](https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-serverless.html)
- [Glue Data Catalog](https://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html)
- [Boto3 Documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)

## ğŸ¤ Support

### Getting Help
1. Check logs trong `logs/` directory
2. Run validation: `python utils/validator.py`
3. Review configuration files
4. Check AWS Console cho resource status

### Common Commands Reference
```bash
# Full deployment
python deploy.py

# Foundation only  
python deploy.py --foundation-only

# Validation
python deploy.py --validate

# Show plan
python deploy.py --plan

# Different environment
python deploy.py --environment prod

# Validation after deployment
python utils/validator.py --environment dev

# Help
python deploy.py --help
```

## ğŸ” Resource Management & Permissions

### Check Required Permissions
```bash
# Check required IAM permissions and get guidance
python permission_checker.py --environment dev

# Verbose output with detailed logging
python permission_checker.py --environment dev --verbose

# Generate minimal IAM policy for destroy operations
python generate_destroy_policy.py
```

### Resource Inspection
```bash
# Inspect current AWS resources without modification
python resource_inspector.py --environment dev

# Different environment  
python resource_inspector.py --environment prod
```

---

**ğŸ¯ Ready to deploy Meta Kaggle Pipeline with pure Python & boto3!**

Há»‡ thá»‘ng nÃ y cung cáº¥p Ä‘áº§y Ä‘á»§ functionality cá»§a Terraform deployment nhÆ°ng vá»›i flexibility vÃ  debugging capabilities cá»§a Python. Perfect cho data engineers muá»‘n cÃ³ control hoÃ n toÃ n over infrastructure deployment process.

ğŸš€ **Complete Infrastructure Lifecycle Management:**
- **Deployment**: Full AWS resource provisioning vá»›i configuration-driven approach
- **Inspection**: Real-time resource discovery vÃ  status checking
- **Permissions**: Pre-flight checks vÃ  detailed guidance for required IAM permissions  
- **Destruction**: Safe cleanup vá»›i confirmation prompts vÃ  reverse dependency ordering
- **Enterprise Ready**: Multi-environment support, cost optimization, comprehensive error handling